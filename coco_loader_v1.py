import pickle as pkimport jsonimport argparsefrom torch.utils.data import Dataset, DataLoaderfrom torchvision import transformsfrom PIL import Imageimport torchimport osimport randomclass Vocabulary(object):    def __init__(self):        self.word2id = {}        self.id2word = {}        self.id = 0    def add_word(self, word):        if word not in self.word2id:            self.word2id[word] = self.id            self.id2word[self.id] = word            self.id += 1    def __call__(self, word):        if word not in self.word2id:            return self.word2id["<UNK>"]        return self.word2id[word]    def __len__(self):        return len(self.word2id)class CocoDataset(Dataset):    def __init__(self, root, json_path, vocab_path, max_length=18, phase='train', transform=None):        self.root = root        assert phase in ['train', 'val', 'test']        self.phase = phase        json_path = os.path.join(root, json_path)        with open(json_path, 'r') as f:            self.json = json.load(f)        vocab_path = os.path.join(root, vocab_path)        with open(vocab_path, 'rb') as f:            self.vocab = pk.load(f)        self.max_length = max_length        if transform is None:            if self.phase == 'train':                self.transforms = transforms.Compose([                    transforms.Resize((224,224)),                    transforms.RandomHorizontalFlip(),                    transforms.ColorJitter(32./255., 0.5, 0.5, 0.032),                    transforms.ToTensor(),                    transforms.Normalize(mean=[0.485, 0.456, 0.406],                                         std=[0.229, 0.224, 0.225])                ])            else:                self.transforms = transforms.Compose([                    transforms.Resize((224,224)),                    transforms.ToTensor(),                    transforms.Normalize(mean=[0.485, 0.456, 0.406],                                         std=[0.229, 0.224, 0.225])                ])        else:            self.transforms = transform    def __getitem__(self, idx):        img_id = self.json[idx]['id']        tokens = self.json[idx]['processed_tokens']        # string captions for evaluation        captions = self.json[idx]['captions']        # make sure the number of captions for each image is match        # assert len(tokens) == 5        assert len(tokens) == len(captions)        img_path = os.path.join(self.root, self.json[idx]['file_path'])        img = Image.open(img_path).convert('RGB')        img = self.transforms(img)        # delete the redundant captions of image where the number of captions is more than 5        if len(captions) != 5:            sample_idx = random.sample(range(0, len(captions)), 5)            tk_sample = []            cap_sample = []            for i in sample_idx:                cap_sample.append(captions[i])                tk_sample.append(tokens[i])            tokens = tk_sample            captions = cap_sample        src_tokens = []        if self.phase != 'test':            tgt_tokens = []            for token in tokens:                source = ['<S>'] + token                # the real length of input is max_length + 1                src_ts = torch.zeros(self.max_length+1).long()                for i, word in enumerate(source):                    if i >= self.max_length + 1:                        break                    src_ts[i] = self.vocab(word)                target = token + ['</S>']                tgt_ts = torch.zeros(self.max_length+1).long()                for i, word in enumerate(target):                    if i >= self.max_length + 1:                        break                    tgt_ts[i] = self.vocab(word)                src_tokens.append(src_ts)                tgt_tokens.append(tgt_ts)            # img: tensor (3 x 224 x 224)            # src_tokens: tensor (5 x 19)            # tgt_tokens: tensor (5 x 19)            # captions : list            # img_id : string            src_tokens = torch.stack(src_tokens, dim=0)            tgt_tokens = torch.stack(tgt_tokens, dim=0)            return img, src_tokens, tgt_tokens, captions, img_id        else:            source = ['<S>']            src_ts = torch.tensor(self.vocab(source[0])).long()            return img, src_ts, captions, img_id    def __len__(self):        return len(self.json)def collate_fn(batch):    imgs = []    captions = []    img_ids = []    if len(batch[0]) == 5:        src_seq = []        tgt_seq = []        for sample in batch:            imgs.append(sample[0])            src_seq.append(sample[1])            tgt_seq.append(sample[2])            captions.append(sample[3])            img_ids.append(sample[4])        imgs = torch.stack(imgs, dim=0)        src_seq = torch.stack(src_seq, dim=0)        tgt_seq = torch.stack(tgt_seq, dim=0)        return imgs, src_seq, tgt_seq, captions, img_ids    elif len(batch[0]) == 4:        src_seq = []        for sample in batch:            imgs.append(sample[0])            src_seq.append(sample[1])            captions.append(sample[2])            img_ids.append(sample[3])        return imgs, src_seq, captions, img_idsdef main():    dataset = CocoDataset('./data/', 'coco_train.json', 'vocab.pkl')    data_loader = DataLoader(dataset=dataset,                             batch_size=12,                             shuffle=True,                             num_workers=6,                             collate_fn=collate_fn)    for batch in data_loader:        imgs, src_seq, tgt_seq, captions, img_ids = batch        print(imgs.size())        print(src_seq.size())        print(tgt_seq.size())        print(len(captions))        print(len(captions[0]))        print(img_ids)    # with open('./data/vocab.pkl','rb') as f:    #     vocab = pk.load(f)    #     print(len(vocab))if __name__ == '__main__':    main()